# [Building A Search Engine] (http://www.udacity.com/overview/Course/cs101/)

====================================


##  Data Structures

### One thing many search engines do is keep track of the number of times a searcher clicks on each link. This is useful for improving ranking results (which we will talk about in Unit 6). Suppose we want to modify our index to keep a count of the number of times a user clicks on each url. Which data structure would be a good way to represent the index?

- (a)

    ```
    [ [ keyword, [ url, url, url, … ], count ], [ keyword, [ url, url, url, … ], count ], … ]
    ```

- (b)

    ```
    [ [ keyword, [ [url, count], [url, count], … ] ], … ]
    ```

- (c)

    ```
    [ [ keyword, url, [count, count, count, ...] ], [ keyword, url, [count, count, count, …] ], … ]
    ```

- (d)

    ```
    [ [ count, keyword, [url, url, … ]], … ]
    ```

**Answer**: (b)


## Ben Bitdiddle

### Ben Bitdiddle suggests changing the index code by replacing the `add_to_index` and lookup procedures with the ones shown below the question.

```python
def add_to_index(index, keyword, url):
    index.append([keyword, url])

def lookup(index, keyword):
    result = []
    for entry in index:
        if entry[0] == keyword:
            result.append(entry[1])
    return result
```

#### This changes the structure of index, but suppose the only way we use index is by calling `add_to_index` and `lookup`. How would this affect the search engine?

- (a) It would produce the wrong results for some lookup queries.
- (b) It would produce the same results for all queries, but `lookup` would sometimes be faster than the original code.
- (c) It would produce the same results for all queries, but `add_to_index` would be faster and `lookup` would usually be slower than the original code.
- (d) It would produce the same results and take the same amount of time for all queries

**Answer**: (c)

## Networking

### It is taking too long for Udacity to upload our lecture videos to YouTube because the video files are many gigabytes of data. What do we need to solve this problem?

- (a) Higher Latency
- (b) Lower Latency
- (c) More Bandwidth
- (d) Less Bandwidth
- (e) More colors in the videos
- (f) Less bad hair in the videos

**Answer**: (c) 

## Latency

### Which one of these would most improve (lower) the latency between Palo Alto, California and Cambridge, Massachusetts? (Note: several of them would improve the latency a little bit, but only the one answer which would produce the biggest improvement is correct.)

- (a) Changing the way messages are encoded to user fewer bits.
- (b) Replacing all the fiber-optic cables with vacuums
- (c) Changing the network so there are only 4 routing points along the page (instead of the 20 needed now).
- (d) Changing the network so there are 100 routing points on the path (instead of the 20 needed now).


**Answer**: (c) 


## Better Splitting

### The built-in `<string>.split()` procedure works okay, but fails to find all the words on a page because it only uses whitespace to split the string. To do better, we should also use punctuation marks to split the page into words.  Define a procedure, `split_string`, that takes two inputs: the string to split and a string containing all of the characters considered separators. The procedure should output a list of strings that break the source string up by the characters in the splitlist.


```python
#out = split_string("This is a test-of the,string separation-code!", " ,!-")
#print out => ['This', 'is', 'a', 'test', 'of', 'the', 'string', 'separation', 'code']

#out = split_string("After  the flood   ...  all the colors came out.", " .")
#print out => ['After', 'the', 'flood', 'all', 'the', 'colors', 'came', 'out']

def split_string(source, splitlist):
    for i in len(splitlist):
        source = source.replace(splitlist[i], ' ')
    return source.split()

```

### Their Solution

```python
def split_string(source, splitlist):
    output = []
    atsplit = True

    for char in source:
        if char in splitlist:
            atsplit = True
        else:
            if atsplit:
                output.append(char)
                atsplit = False
            else:
                output[-1] = output[-1] + char

        return output
```


## Improving the Index

### The current index includes a url in the list of urls for a keyword multiple times if the keyword appears on that page more than once. It might be better to only include the same url once in the url list for a keyword, even if it appears many times. Modify `add_to_index` so that a given url is only included once in the url list for a keyword, no matter how many times that keyword appears.

```python
#index = crawl_web("http://www.udacity.com/cs101x/index.html")
#print lookup(index,"is") => ['http://www.udacity.com/cs101x/index.html']

def add_to_index(index, keyword, url):
    for entry in index:
        if entry[0] == keyword:
            if url not in entry[1]:
                entry[1].append(url)
            return
    # not found, add new keyword to index
    index.append([keyword, [url]])


def get_page(url):
    try:
        if url == "http://www.udacity.com/cs101x/index.html":
            return  '<html> <body> This is a test page for learning to crawl! <p> It is a good idea to  <a href="http://www.udacity.com/cs101x/crawling.html">learn to crawl</a> before you try to  <a href="http://www.udacity.com/cs101x/walking.html">walk</a> or  <a href="http://www.udacity.com/cs101x/flying.html">fly</a>. </p> </body> </html> '
        elif url == "http://www.udacity.com/cs101x/crawling.html":
            return  '<html> <body> I have not learned to crawl yet, but I am quite good at  <a href="http://www.udacity.com/cs101x/kicking.html">kicking</a>. </body> </html>'
        elif url == "http://www.udacity.com/cs101x/walking.html":
            return '<html> <body> I cant get enough  <a href="http://www.udacity.com/cs101x/index.html">crawling</a>! </body> </html>'
        elif url == "http://www.udacity.com/cs101x/flying.html":
            return '<html> <body> The magic words are Squeamish Ossifrage! </body> </html>'
    except:
        return ""
    return ""

def union(a, b):
    for e in b:
        if e not in a:
            a.append(e)

def get_next_target(page):
    start_link = page.find('<a href=')
    if start_link == -1: 
        return None, 0
    start_quote = page.find('"', start_link)
    end_quote = page.find('"', start_quote + 1)
    url = page[start_quote + 1:end_quote]
    return url, end_quote

def get_all_links(page):
    links = []
    while True:
        url, endpos = get_next_target(page)
        if url:
            links.append(url)
            page = page[endpos:]
        else:
            break
    return links

def crawl_web(seed):
    tocrawl = [seed]
    crawled = []
    index = []
    while tocrawl: 
        page = tocrawl.pop()
        if page not in crawled:
            content = get_page(page)
            add_page_to_index(index, page, content)
            union(tocrawl, get_all_links(content))
            crawled.append(page)
    return index

def add_page_to_index(index, url, content):
    words = content.split()
    for word in words:
        add_to_index(index, word, url)

def lookup(index, keyword):
    for entry in index:
        if entry[0] == keyword:
            return entry[1]
    return None
```


## Counting Clicks

### One way search engines rank pages is to count the number of times a searcher clicks on a returned link. This indicates that the person doing the query thought this was a useful link for the query, so it should be higher in the rankings next time. Modify the index such that for each url in a list for a keyword, there is also a number that counts the number of times a user clicks on that link for this keyword.

### The result of `lookup(index,keyword)` should now be a list of url entries, where each url entry is a list of a url and a number indicating the number of times that url was clicked for this query keyword. You should define a new procedure to simulate user clicks for a given link: `record_user_click(index,word,url)` that modifies the entry in the index for the input word by increasing the count associated with the url by 1.

### You also will have to modify `add_to_index` in order to correctly create the new data structure. Here is an example showing a sequence of interactions:

```python
index = crawl_web('http://www.udacity.com/cs101x/index.html')
print lookup(index, 'good') #=> [['http://www.udacity.com/cs101x/index.html', 0], ['http://www.udacity.com/cs101x/crawling.html', 0]]
record_user_click(index, 'good', 'http://www.udacity.com/cs101x/crawling.html')
print lookup(index, 'good') #=> [['http://www.udacity.com/cs101x/index.html', 0], ['http://www.udacity.com/cs101x/crawling.html', 1]]

def record_user_click(index, keyword, url):
    for i in index:
        if i[0] == keyword:
            for j in i[1]:
                if j[0] == url:
                    j[1] += 1

def add_to_index(index, keyword, url):
    for entry in index:
        if entry[0] == keyword:
            entry[1].append([url, 0])
            return
    index.append([keyword, [[url, 0]]])


def get_page(url):
    try:
        if url == "http://www.udacity.com/cs101x/index.html":
            return  '<html> <body> This is a test page for learning to crawl! <p> It is a good idea to  <a href="http://www.udacity.com/cs101x/crawling.html">learn to crawl</a> before you try to  <a href="http://www.udacity.com/cs101x/walking.html">walk</a> or  <a href="http://www.udacity.com/cs101x/flying.html">fly</a>. </p> </body> </html> '
        elif url == "http://www.udacity.com/cs101x/crawling.html":
            return  '<html> <body> I have not learned to crawl yet, but I am quite good at  <a href="http://www.udacity.com/cs101x/kicking.html">kicking</a>. </body> </html>'
        elif url == "http://www.udacity.com/cs101x/walking.html":
            return '<html> <body> I cant get enough  <a href="http://www.udacity.com/cs101x/index.html">crawling</a>! </body> </html>'
        elif url == "http://www.udacity.com/cs101x/flying.html":
            return '<html> <body> The magic words are Squeamish Ossifrage! </body> </html>'
    except:
        return ""
    return ""

def union(a, b):
    for e in b:
        if e not in a:
            a.append(e)

def get_next_target(page):
    start_link = page.find('<a href=')
    if start_link == -1: 
        return None, 0
    start_quote = page.find('"', start_link)
    end_quote = page.find('"', start_quote + 1)
    url = page[start_quote + 1:end_quote]
    return url, end_quote

def get_all_links(page):
    links = []
    while True:
        url, endpos = get_next_target(page)
        if url:
            links.append(url)
            page = page[endpos:]
        else:
            break
    return links

def crawl_web(seed):
    tocrawl = [seed]
    crawled = []
    index = []
    while tocrawl: 
        page = tocrawl.pop()
        if page not in crawled:
            content = get_page(page)
            add_page_to_index(index, page, content)
            union(tocrawl, get_all_links(content))
            crawled.append(page)
    return index

def add_page_to_index(index, url, content):
    words = content.split()
    for word in words:
        add_to_index(index, word, url)

def lookup(index, keyword):
    for entry in index:
        if entry[0] == keyword:
            return entry[1]
    return None
```
